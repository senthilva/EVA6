{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_1cycle_lr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wd2ZSmAasvF"
      },
      "source": [
        "# Super convergence using 1 Cycle LR for CIFAR 10\n",
        "\n",
        "The goal here is to write a custom ResNet architecture for CIFAR10 to achive 90+ accuracy in less than 24 epochs using 1 cycle LR strategy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceAs8Ds6AOk5"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Chqtsuxn4te"
      },
      "source": [
        "# Install Albumentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKlMtcEI07oQ",
        "outputId": "eaeaf5cc-955c-4fc1-b8b3-524893e8c369"
      },
      "source": [
        "\n",
        "!pip install --upgrade --force-reinstall --no-deps albumentations"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/2fa0ec5e0c04c410cbb54dd79910afa884409440653aa4688654e6497e2a/albumentations-1.0.2-py3-none-any.whl (98kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 25.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20kB 32.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 40kB 37.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 51kB 31.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61kB 33.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 71kB 33.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 81kB 33.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 13.0MB/s \n",
            "\u001b[?25hInstalling collected packages: albumentations\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhBZceWOB7gk",
        "outputId": "3c4076e8-1c51-45fe-cf5f-bf7b318bd95e"
      },
      "source": [
        "!git clone https://github.com/senthilva/deeplearning_template.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearning_template'...\n",
            "remote: Enumerating objects: 278, done.\u001b[K\n",
            "remote: Counting objects: 100% (278/278), done.\u001b[K\n",
            "remote: Compressing objects: 100% (267/267), done.\u001b[K\n",
            "remote: Total 278 (delta 106), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (278/278), 65.55 KiB | 4.37 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5cenxK1ayFM"
      },
      "source": [
        "# Change Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj-9FtS3EKVB",
        "outputId": "fa734080-41b8-4b88-949d-3852fcc0f26c"
      },
      "source": [
        "%cd deeplearning_template"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deeplearning_template\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r62b6RQa0fF"
      },
      "source": [
        "# Display folder contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-oRf8SNKvJT",
        "outputId": "345f4345-30fd-429b-98f5-c8168420d536"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "main.py  models  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHfesFMga6t_"
      },
      "source": [
        "## Add to system path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Fb4dYjChiD"
      },
      "source": [
        "#%cd deeplearning_template\n",
        "import sys\n",
        "sys.path.insert(0,'/content/deeplearning_template')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWynaUoea9Rm"
      },
      "source": [
        "# Run the main program : 24 epochs and batch size  512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDTQk1ISDhdQ",
        "outputId": "c4754af9-51d9-4516-999a-33b725a6524f"
      },
      "source": [
        "!python main.py --no_of_epochs 24 --batch_size 512"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:690: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "170499072it [00:02, 84628007.10it/s]                   \n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            " [================================================================>]  Step: 1s271ms | Tot: 14s515ms | Loss: 2.119 | Acc: 33.636% (16818/50000) 98/98 \n",
            " [=============================================================>...]  Step: 393ms | Tot: 1s191ms | Loss: 2.074 | Acc: 38.090% (3809/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.05865921643637467]\n",
            "\n",
            "Epoch: 2\n",
            " [================================================================>]  Step: 88ms | Tot: 13s477ms | Loss: 2.033 | Acc: 42.238% (21119/50000) 98/98 \n",
            " [=============================================================>...]  Step: 23ms | Tot: 822ms | Loss: 2.030 | Acc: 42.670% (4267/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.1676805246312752]\n",
            "\n",
            "Epoch: 3\n",
            " [================================================================>]  Step: 90ms | Tot: 13s434ms | Loss: 2.027 | Acc: 42.936% (21468/50000) 98/98 \n",
            " [=============================================================>...]  Step: 23ms | Tot: 830ms | Loss: 2.059 | Acc: 40.100% (4010/10000) 20/20 \n",
            " Learning Rate : [0.2946184706007123]\n",
            "\n",
            "Epoch: 4\n",
            " [================================================================>]  Step: 91ms | Tot: 13s663ms | Loss: 2.011 | Acc: 44.626% (22313/50000) 98/98 \n",
            " [=============================================================>...]  Step: 24ms | Tot: 847ms | Loss: 1.991 | Acc: 46.730% (4673/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.38306604068740413]\n",
            "\n",
            "Epoch: 5\n",
            " [================================================================>]  Step: 93ms | Tot: 13s801ms | Loss: 1.963 | Acc: 49.442% (24721/50000) 98/98 \n",
            " [=============================================================>...]  Step: 25ms | Tot: 846ms | Loss: 1.939 | Acc: 51.790% (5179/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.399684520617609]\n",
            "\n",
            "Epoch: 6\n",
            " [================================================================>]  Step: 95ms | Tot: 14s8ms | Loss: 1.924 | Acc: 53.350% (26675/50000) 98/98 \n",
            " [=============================================================>...]  Step: 31ms | Tot: 872ms | Loss: 1.930 | Acc: 52.830% (5283/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.39581087341411936]\n",
            "\n",
            "Epoch: 7\n",
            " [================================================================>]  Step: 96ms | Tot: 14s269ms | Loss: 1.883 | Acc: 57.590% (28795/50000) 98/98 \n",
            " [=============================================================>...]  Step: 32ms | Tot: 902ms | Loss: 1.868 | Acc: 59.350% (5935/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.3876063425675938]\n",
            "\n",
            "Epoch: 8\n",
            " [================================================================>]  Step: 99ms | Tot: 14s829ms | Loss: 1.811 | Acc: 64.984% (32492/50000) 98/98 \n",
            " [=============================================================>...]  Step: 28ms | Tot: 936ms | Loss: 1.806 | Acc: 65.470% (6547/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.37525239406608124]\n",
            "\n",
            "Epoch: 9\n",
            " [================================================================>]  Step: 100ms | Tot: 14s579ms | Loss: 1.782 | Acc: 67.964% (33982/50000) 98/98 \n",
            " [=============================================================>...]  Step: 28ms | Tot: 901ms | Loss: 1.818 | Acc: 64.440% (6444/10000) 20/20 \n",
            " Learning Rate : [0.35902226979422636]\n",
            "\n",
            "Epoch: 10\n",
            " [================================================================>]  Step: 96ms | Tot: 14s265ms | Loss: 1.739 | Acc: 72.766% (36383/50000) 98/98 \n",
            " [=============================================================>...]  Step: 28ms | Tot: 881ms | Loss: 1.766 | Acc: 69.720% (6972/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.33927494403005204]\n",
            "\n",
            "Epoch: 11\n",
            " [================================================================>]  Step: 97ms | Tot: 14s195ms | Loss: 1.714 | Acc: 75.444% (37722/50000) 98/98 \n",
            " [=============================================================>...]  Step: 30ms | Tot: 862ms | Loss: 1.780 | Acc: 68.190% (6819/10000) 20/20 \n",
            " Learning Rate : [0.31644718373173647]\n",
            "\n",
            "Epoch: 12\n",
            " [================================================================>]  Step: 96ms | Tot: 14s199ms | Loss: 1.701 | Acc: 76.626% (38313/50000) 98/98 \n",
            " [=============================================================>...]  Step: 31ms | Tot: 895ms | Loss: 1.706 | Acc: 76.020% (7602/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.29104388822319777]\n",
            "\n",
            "Epoch: 13\n",
            " [================================================================>]  Step: 97ms | Tot: 14s384ms | Loss: 1.684 | Acc: 78.474% (39237/50000) 98/98 \n",
            " [=============================================================>...]  Step: 29ms | Tot: 890ms | Loss: 1.700 | Acc: 77.030% (7703/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.26362692194316906]\n",
            "\n",
            "Epoch: 14\n",
            " [================================================================>]  Step: 96ms | Tot: 14s482ms | Loss: 1.672 | Acc: 79.676% (39838/50000) 98/98 \n",
            " [=============================================================>...]  Step: 29ms | Tot: 896ms | Loss: 1.691 | Acc: 78.170% (7817/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.23480268725253775]\n",
            "\n",
            "Epoch: 15\n",
            " [================================================================>]  Step: 95ms | Tot: 14s345ms | Loss: 1.666 | Acc: 80.536% (40268/50000) 98/98 \n",
            " [=============================================================>...]  Step: 32ms | Tot: 876ms | Loss: 1.677 | Acc: 79.290% (7929/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.20520871216182993]\n",
            "\n",
            "Epoch: 16\n",
            " [================================================================>]  Step: 98ms | Tot: 14s319ms | Loss: 1.654 | Acc: 81.662% (40831/50000) 98/98 \n",
            " [=============================================================>...]  Step: 30ms | Tot: 885ms | Loss: 1.664 | Acc: 80.860% (8086/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.17549954962850084]\n",
            "\n",
            "Epoch: 17\n",
            " [================================================================>]  Step: 95ms | Tot: 14s255ms | Loss: 1.644 | Acc: 82.696% (41348/50000) 98/98 \n",
            " [=============================================================>...]  Step: 34ms | Tot: 877ms | Loss: 1.679 | Acc: 78.940% (7894/10000) 20/20 \n",
            " Learning Rate : [0.1463323003002368]\n",
            "\n",
            "Epoch: 18\n",
            " [================================================================>]  Step: 97ms | Tot: 14s320ms | Loss: 1.635 | Acc: 83.670% (41835/50000) 98/98 \n",
            " [=============================================================>...]  Step: 25ms | Tot: 937ms | Loss: 1.652 | Acc: 81.710% (8171/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.1183520789090106]\n",
            "\n",
            "Epoch: 19\n",
            " [================================================================>]  Step: 98ms | Tot: 14s401ms | Loss: 1.625 | Acc: 84.734% (42367/50000) 98/98 \n",
            " [=============================================================>...]  Step: 26ms | Tot: 964ms | Loss: 1.662 | Acc: 80.700% (8070/10000) 20/20 \n",
            " Learning Rate : [0.0921777457669548]\n",
            "\n",
            "Epoch: 20\n",
            " [================================================================>]  Step: 96ms | Tot: 14s328ms | Loss: 1.616 | Acc: 85.568% (42784/50000) 98/98 \n",
            " [=============================================================>...]  Step: 26ms | Tot: 967ms | Loss: 1.636 | Acc: 83.270% (8327/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.06838821895165247]\n",
            "\n",
            "Epoch: 21\n",
            " [================================================================>]  Step: 96ms | Tot: 14s414ms | Loss: 1.603 | Acc: 86.728% (43364/50000) 98/98 \n",
            " [=============================================================>...]  Step: 26ms | Tot: 978ms | Loss: 1.617 | Acc: 85.220% (8522/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.04750966992488575]\n",
            "\n",
            "Epoch: 22\n",
            " [================================================================>]  Step: 98ms | Tot: 14s486ms | Loss: 1.592 | Acc: 87.914% (43957/50000) 98/98 \n",
            " [=============================================================>...]  Step: 25ms | Tot: 935ms | Loss: 1.607 | Acc: 86.420% (8642/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.0300038857892982]\n",
            "\n",
            "Epoch: 23\n",
            " [================================================================>]  Step: 96ms | Tot: 14s383ms | Loss: 1.578 | Acc: 89.338% (44669/50000) 98/98 \n",
            " [=============================================================>...]  Step: 25ms | Tot: 953ms | Loss: 1.589 | Acc: 88.060% (8806/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.016258055583989996]\n",
            "\n",
            "Epoch: 24\n",
            " [================================================================>]  Step: 96ms | Tot: 14s359ms | Loss: 1.570 | Acc: 90.134% (45067/50000) 98/98 \n",
            " [=============================================================>...]  Step: 30ms | Tot: 889ms | Loss: 1.583 | Acc: 88.590% (8859/10000) 20/20 \n",
            "Saving..\n",
            " Learning Rate : [0.006576206523486491]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3olGAHJ7Hrt"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}